{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPkAsc0rpobN6QEoahI5dL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ELiTE0005/NNML-algorithms/blob/main/Candidate_Elimination.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzXYNHEnol5D",
        "outputId": "c6196c07-7ff6-4a88-8518-5afb8f8a7452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Specific Hypothesis (S): ('?', '?', '?', '?', '?', '?', '?', '?', '?')\n",
            "General Hypothesis Set (G): []\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"cybersecurity_intrusion_data.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert numerical features into categorical bins\n",
        "numeric_features = [\"network_packet_size\", \"login_attempts\", \"session_duration\",\n",
        "                    \"ip_reputation_score\", \"failed_logins\", \"unusual_time_access\"]\n",
        "\n",
        "for feature in numeric_features:\n",
        "    unique_vals = df[feature].nunique()\n",
        "    if unique_vals > 4:  # Use fixed binning for better stability\n",
        "        df[feature] = pd.cut(df[feature], bins=4, labels=[\"low\", \"medium\", \"high\", \"very_high\"], include_lowest=True)\n",
        "    elif unique_vals > 2:\n",
        "        df[feature] = pd.cut(df[feature], bins=unique_vals, labels=[str(i) for i in range(unique_vals)], include_lowest=True)\n",
        "    else:\n",
        "        df[feature] = df[feature].astype(str)  # Convert binary features to categorical strings\n",
        "\n",
        "# Drop session_id if present\n",
        "df = df.drop(columns=[\"session_id\"], errors='ignore')\n",
        "\n",
        "# Extract features (X) and target variable (y)\n",
        "X = df.drop(columns=[\"attack_detected\"]).values\n",
        "y = df[\"attack_detected\"].values\n",
        "\n",
        "# Initialize S (Specific boundary) with the first positive example\n",
        "S = X[np.where(y == 1)][0].copy()\n",
        "\n",
        "# Initialize G (General boundary) with the most general hypothesis\n",
        "G = [[\"?\" for _ in range(len(S))]]\n",
        "\n",
        "# Candidate Elimination Algorithm\n",
        "for i in range(len(X)):\n",
        "    instance, label = X[i], y[i]\n",
        "\n",
        "    if label == 1:  # Positive example\n",
        "        # Generalize S to be consistent with instance\n",
        "        for j in range(len(S)):\n",
        "            if S[j] != instance[j]:\n",
        "                S[j] = \"?\"\n",
        "\n",
        "        # Remove inconsistent hypotheses from G\n",
        "        G = [g for g in G if all(g[k] == \"?\" or g[k] == instance[k] for k in range(len(S)))]\n",
        "\n",
        "    elif label == 0:  # Negative example\n",
        "        # Remove inconsistent hypotheses from S\n",
        "        S = [S[j] if S[j] == instance[j] else \"?\" for j in range(len(S))]\n",
        "\n",
        "        # Specialize G to exclude the instance\n",
        "        new_G = []\n",
        "        for g in G:\n",
        "            for j in range(len(g)):\n",
        "                if g[j] == \"?\":\n",
        "                    specialized_hypothesis = g.copy()\n",
        "                    specialized_hypothesis[j] = instance[j]\n",
        "                    new_G.append(specialized_hypothesis)\n",
        "\n",
        "        G = new_G\n",
        "\n",
        "# Convert to unique sets for final hypothesis space\n",
        "S_final = tuple(S)\n",
        "G_final = [tuple(g) for g in G]\n",
        "\n",
        "print(\"Specific Hypothesis (S):\", S_final)\n",
        "print(\"General Hypothesis Set (G):\", G_final)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "27O5xZgtoxT4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}